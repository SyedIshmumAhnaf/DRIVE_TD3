main_td3.py:
import torch
import numpy as np
from RLlib.td3_agent import DeterministicActor, TwinCritic
from src.environment import DashCamEnv
from src.DADALoader import DADALoader
from RLlib.replay_buffer import ReplayMemory, ReplayMemoryGPU  # Reuse existing buffer
from trainers.td3_trainer import TD3Trainer
import yaml
from src.DADALoader import setup_dataloader

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

with open("cfgs/td3_mlnet.yml", "r") as f:
    cfg = yaml.safe_load(f)

def train_td3(cfg):
    # Initialize environment and dataset
    #env = DashCamEnv(cfg)
    env = DashCamEnv(cfg, device)
    #dataset = DADALoader(cfg.data_root, cfg.mode, cfg.frame_interval)
    dataset = DADALoader(cfg["data_path"], cfg["mode"], cfg["frame_interval"])

    
    # TD3 components
    state_dim = 128  # From DRIVE's RAE encoder
    action_dim = 3   # [accident_score, x, y]
    actor = DeterministicActor(state_dim, action_dim).to(device)
    critic = TwinCritic(state_dim, action_dim).to(device)
    #replay_buffer = ReplayMemory(cfg.buffer_size)
    replay_buffer = ReplayMemoryGPU(cfg, device=device)
    #replay_buffer = ReplayMemoryGPU(replay_size=cfg["replay_size"], device=device)

    trainer = TD3Trainer(
    actor=actor.to(device), 
    critic=critic.to(device),
    gamma=cfg["gamma"],
    tau=cfg["tau"],
    policy_noise=cfg["policy_noise"],
    noise_clip=cfg["noise_clip"],
    policy_freq=cfg["policy_freq"]
    )
    
    # Trainer (from previous TD3Trainer class)
    #trainer = TD3Trainer(actor, critic, gamma=cfg['gamma'], tau=cfg['tau'],
    #                     policy_noise=0.2, noise_clip=0.5, policy_freq=2)
    
    # Training loop
    for episode in range(cfg['epochs']):
        #video_data, coord_data, data_info = dataset.sample_batch(cfg['batch_size'])
        dataloader = setup_dataloader(cfg)  # ✅ Get the data loader
        for batch in dataloader:
            video_data, coord_data, data_info, *_ = batch
            break  # ✅ Only take one batch per episode
        state = env.set_data(video_data, coord_data, data_info)
        
        for step in range(env.max_steps):
            # Select action with exploration noise
            with torch.no_grad():
                action = actor(state)
                #noise = torch.randn_like(action) * cfg.exploration_noise
                noise = torch.randn_like(action) * cfg["exploration_noise"]
                action = (action + noise).clamp(-1, 1)
            
            # Step environment
            next_state, reward, _ = env.step(action)
            done = (step == env.max_steps - 1)
            
            # Store transitiong
            replay_buffer.add(
                state.to(device),
                action.to(device),
                reward.to(device),
                next_state.to(device),
                done
            )
            state = next_state
            
            # Train after collecting sufficient samples
            if len(replay_buffer) > cfg['batch_size']:
                trainer.update(replay_buffer, cfg['batch_size'])

            critic_loss, actor_loss = trainer.update(replay_buffer, cfg['batch_size'])
            
            if step % 100 == 0:
                print(f"Step: {step}, Critic Loss: {critic_loss:.3f}, Actor Loss: {actor_loss:.3f}")
        
        # Periodic evaluation
        if episode % cfg["eval_interval"] == 0:
            test_performance(actor, env, device)
            
def test_performance(actor, env, device):
    # Run agent without exploration noise
    with torch.no_grad():
        state = env.reset()
        total_reward = 0
        for _ in range(env.max_steps):
            action = actor(state)
            state, reward, _ = env.step(action, isTraining=False)
            total_reward += reward.mean().item()
    print(f"Test Reward: {total_reward:.2f}")

if __name__ == "__main__":
    #from cfgs.td3_mlnet import cfg  # Your TD3 config file
    train_td3(cfg)

environment.py:
class DashCamEnv(core.Env):
    def __init__(self, cfg, device=torch.device("cuda")):
        self.device = device
        #self.saliency = cfg.saliency
        self.saliency = cfg["saliency"]
        if self.saliency == 'MLNet':
            self.observe_model = MLNet(cfg["input_shape"])
        elif self.saliency == 'TASED-Net':
            self.observe_model = TASED_v2(cfg["input_shape"])
        else:
            raise NotImplementedError
        self.output_shape = self.observe_model.output_shape  # (60, 80)
        self.foveal_model = TorchFovea(cfg["input_shape"], min(cfg["input_shape"])/6.0, level=5, factor=2, device=device)
        self.len_clip = cfg["len_clip"]
        self.input_size = cfg["input_shape"]  # (480, 640)
        self.image_size = cfg["image_shape"]  # (330, 792)
        self.fps = 30 / cfg["frame_interval"]
        self.step_size = cfg["step_size"]
        self.score_thresh = cfg["score_thresh"]
        self.state_norm = cfg["state_norm"]
        self.fusion = cfg["fusion"]
        self.fusion_margin = cfg["fusion_margin"]
        self.rho = cfg["rho"]
        self.use_salmap = cfg["use_salmap"]

    def set_model(self, pretrained=False, weight_file=None):
        if pretrained and weight_file is not None:
            # load model weight file
            assert os.path.exists(weight_file), "Checkpoint directory does not exist! %s"%(weight_file)
            ckpt = torch.load(weight_file)
            if self.saliency == 'MLNet':
                self.observe_model.load_state_dict(ckpt['model'])
            elif self.saliency == 'TASED-Net':
                model_dict = self.observe_model.state_dict()
                for name, param in ckpt.items():
                    if 'module' in name:
                        name = '.'.join(name.split('.')[1:])
                    if name in model_dict:
                        if param.size() == model_dict[name].size():
                            model_dict[name].copy_(param)
                        else:
                            print (' size? ' + name, param.size(), model_dict[name].size())
                    else:
                        print (' name? ' + name)
                self.observe_model.load_state_dict(model_dict)
            else:
                raise NotImplementedError
            self.observe_model.to(self.device)
            self.observe_model.eval()
        else:
            self.observe_model.to(self.device)
            self.observe_model.train()

    def step(self, actions, isTraining=True):
        """ actions: (B, 3)
        """
        batch_size = actions.size(0)
        # parse actions (current accident scores, the next attention mask)
        score_pred = 0.5 * (actions[:, 0] + 1.0)  # map to [0, 1], shape=(B,)
        fix_pred = scales_to_point(actions[:, 1:], self.image_size, self.input_size)  # (B, 2)  (x,y)

        # update rho (dynamic)
        if self.fusion == 'dynamic':
            self.rho = torch.clamp_max(score_pred.clone(), self.fusion_margin)  # (B,)

        info = {}
        if not isTraining:
            info.update({'pred_score': score_pred, 'pred_fixation': fix_pred})

        if self.cur_step < self.max_steps - 1:  # cur_step starts from 0
            # next state
            next_state = self.get_next_state(fix_pred, self.cur_step + 1)
            # reward (immediate)
            cur_rewards = self.get_reward(score_pred, fix_pred) if isTraining else 0
        else:
            # The last step
            next_state = self.cur_state.clone()  # GPU array
            cur_rewards = torch.zeros([batch_size, 1], dtype=torch.float32).to(self.device) if isTraining else 0

        self.cur_step += 1
        self.cur_state = next_state.clone()

        return next_state, cur_rewards, info

td3_trainer.py:
import torch
import copy
import numpy as np

class TD3Trainer:
    def __init__(self, actor, critic, gamma=0.99, tau=0.005, policy_noise=0.2, 
                 noise_clip=0.5, policy_freq=2, actor_lr=1e-4, critic_lr=1e-3):
        """
        Twin Delayed Deep Deterministic Policy Gradients (TD3)
        
        Args:
            actor: Deterministic policy network
            critic: Twin Q-network
            gamma: Discount factor
            tau: Target network update rate
            policy_noise: Std of noise added to target actions
            noise_clip: Noise clipping range
            policy_freq: Frequency of delayed policy updates
        """
        self.actor = actor
        self.critic = critic
        self.gamma = gamma
        self.tau = tau
        self.policy_noise = policy_noise
        self.noise_clip = noise_clip
        self.policy_freq = policy_freq
        
        # Initialize target networks
        self.actor_target = copy.deepcopy(actor)
        self.critic_target = copy.deepcopy(critic)
        
        # Optimizers
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)
        
        self.total_steps = 0

    def update(self, replay_buffer, batch_size):
        """Update policy and value parameters"""
        # Sample replay buffer
        state, action, reward, next_state, done = replay_buffer.sample(batch_size)
        
        with torch.no_grad():
            # Add clipped noise to target actions
            noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)
            next_action = (self.actor_target(next_state) + noise).clamp(-1, 1)
            
            # Compute target Q-values (min of twin critics)
            target_Q1, target_Q2 = self.critic_target(next_state, next_action)
            target_Q = torch.min(target_Q1, target_Q2)
            target_Q = reward + (1 - done) * self.gamma * target_Q

        # Get current Q estimates
        current_Q1, current_Q2 = self.critic(state, action)
        
        # Critic loss
        critic_loss = torch.nn.functional.mse_loss(current_Q1, target_Q) + \
                      torch.nn.functional.mse_loss(current_Q2, target_Q)
        
        # Optimize critics
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)  # Gradient clipping
        self.critic_optimizer.step()
        
        # Delayed policy updates
        if self.total_steps % self.policy_freq == 0:
            # Actor loss (maximize Q1)
            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()
            
            # Optimize actor
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
            self.actor_optimizer.step()
            
            # Update target networks
            self._soft_update(self.actor_target, self.actor)
            self._soft_update(self.critic_target, self.critic)
        
        self.total_steps += 1
        return critic_loss.item(), actor_loss.item() if (self.total_steps % self.policy_freq == 0) else 0.0

    def _soft_update(self, target, source):
        """Soft update target networks"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)

    def save(self, filename):
        """Save models to file"""
        torch.save({
            'actor': self.actor.state_dict(),
            'critic': self.critic.state_dict(),
            'actor_target': self.actor_target.state_dict(),
            'critic_target': self.critic_target.state_dict(),
            'actor_optimizer': self.actor_optimizer.state_dict(),
            'critic_optimizer': self.critic_optimizer.state_dict(),
        }, filename)

    def load(self, filename):
        """Load models from file"""
        checkpoint = torch.load(filename)
        self.actor.load_state_dict(checkpoint['actor'])
        self.critic.load_state_dict(checkpoint['critic'])
        self.actor_target.load_state_dict(checkpoint['actor_target'])
        self.critic_target.load_state_dict(checkpoint['critic_target'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])

td3_agent.py:
class DeterministicActor(nn.Module):
    """TD3 Actor: Predicts accident score + fixation point (deterministic)"""
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.action_head = nn.Linear(hidden_dim, action_dim)
        
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        actions = self.action_head(x)
        # Accident score: [0, 1] via sigmoid | Fixation: [-1, 1] via tanh
        accident_score = torch.sigmoid(actions[:, 0].unsqueeze(1))  # (B, 1)
        fixation = torch.tanh(actions[:, 1:])                       # (B, 2)
        return torch.cat([accident_score, fixation], dim=1)